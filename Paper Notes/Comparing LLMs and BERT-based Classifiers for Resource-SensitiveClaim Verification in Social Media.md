#factchecking #socialmedia #bert #LLM

*link*: 

- Context: transformer X LLM
	https://epium.com/llm-vs-transformer-understanding-key-differences/
	- Transformer: neural network structure that has self-attention mechanisms, positional encoding, paralell processing
	- LLM: **applications** **built** **on top of transformer architecture**;  large-scale, text-focused model that uses trnasformers for language understanding and generation
- Applying LLMs in the context of social media is problematic: **high computational costs and latency** 
- Encoder only finetuned BERT-based models have lower computational costs
### **Methodology**: 
- Compare encoder-only BERT X decoder-only zero-shot LLMs

![[comparing_LLMs_BERT.pdf]]